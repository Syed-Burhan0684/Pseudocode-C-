{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ae62f55",
      "metadata": {
        "id": "4ae62f55"
      },
      "source": [
        "# LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "fdf35712",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdf35712",
        "outputId": "4b6c7dfa-b8a9-4f05-e0cf-53967072a864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              text  \\\n",
            "0                in the function gcd(a,b=integers)   \n",
            "1  if b=1 return a, else call function gcd(b, a%b)   \n",
            "2                                              NaN   \n",
            "3                                              NaN   \n",
            "4               n , nn, ans = integers with ans =0   \n",
            "\n",
            "                             code  workerid probid       subid  line  indent  \n",
            "0         int gcd(int a, int b) {        38    13A  41120785.0   0.0     0.0  \n",
            "1  return !b ? a : gcd(b, a % b);        38    13A  41120785.0   1.0     1.0  \n",
            "2                               }        38    13A  41120785.0   2.0     0.0  \n",
            "3                    int main() {        38    13A  41120785.0   3.0     0.0  \n",
            "4             int n, nn, ans = 0;        38    13A  41120785.0   4.0     1.0  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download both 'punkt' and 'punkt_tab' tokenizer data\n",
        "nltk.download('punkt')      # For compatibility with older NLTK versions\n",
        "nltk.download('punkt_tab')  # Required for newer NLTK versions\n",
        "\n",
        "# Load dataset with tab-separated values\n",
        "file_path = r\"/content/spoc-train.tsv\"  # Change to your dataset's path\n",
        "\n",
        "# Use sep='\\t' to correctly read TSV files\n",
        "df = pd.read_csv(file_path, sep='\\t', on_bad_lines='skip')\n",
        "\n",
        "# Display dataset information\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6889f401",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6889f401",
        "outputId": "42f7c7bf-b682-47eb-9178-e0891f106ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 109002 entries, 0 to 109001\n",
            "Data columns (total 7 columns):\n",
            " #   Column    Non-Null Count   Dtype  \n",
            "---  ------    --------------   -----  \n",
            " 0   text      80289 non-null   object \n",
            " 1   code      109002 non-null  object \n",
            " 2   workerid  109002 non-null  int64  \n",
            " 3   probid    109002 non-null  object \n",
            " 4   subid     109001 non-null  float64\n",
            " 5   line      109001 non-null  float64\n",
            " 6   indent    109001 non-null  float64\n",
            "dtypes: float64(3), int64(1), object(3)\n",
            "memory usage: 5.8+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(df.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd3ad88e",
      "metadata": {
        "id": "dd3ad88e"
      },
      "source": [
        "# DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3a08429f",
      "metadata": {
        "id": "3a08429f"
      },
      "outputs": [],
      "source": [
        "# Convert 'text' column to string and tokenize\n",
        "df[\"text\"] = df[\"text\"].astype(str)\n",
        "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "80e52203",
      "metadata": {
        "id": "80e52203"
      },
      "outputs": [],
      "source": [
        "df[\"text\"] = df[\"text\"].fillna(\"\")  # Replace NaN with empty strings\n",
        "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3e73cd24",
      "metadata": {
        "id": "3e73cd24"
      },
      "outputs": [],
      "source": [
        "df[\"text_tokens\"] = df[\"text\"].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "123f02bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "123f02bd",
        "outputId": "cd2ae680-d819-4547-e689-5f5828a1cf88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object\n",
            "0\n",
            "Empty DataFrame\n",
            "Columns: [text, code, workerid, probid, subid, line, indent, text_tokens]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "print(df[\"text\"].dtype)  # Check the column's data type\n",
        "print(df[\"text\"].isna().sum())  # Count missing values\n",
        "print(df[df[\"text\"].apply(lambda x: not isinstance(x, str))])  # Show non-string values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7666823c",
      "metadata": {
        "id": "7666823c"
      },
      "source": [
        "# TOKENIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1527b229",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1527b229",
        "outputId": "bcd13e9f-3b3b-4715-e913-62d106ea61a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download tokenizer if not available\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenizing pseudocode and code\n",
        "df[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)\n",
        "df[\"code_tokens\"] = df[\"code\"].apply(word_tokenize)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e8e137",
      "metadata": {
        "id": "62e8e137"
      },
      "source": [
        "# PRINT TOKEN SAMPLES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3667d51e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3667d51e",
        "outputId": "5104a9fa-7d16-493e-b35d-958f644b3d14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples from index 1-10:\n",
            "Index 1:\n",
            "Tokenized Pseudocode: ['if', 'b=1', 'return', 'a', ',', 'else', 'call', 'function', 'gcd', '(', 'b', ',', 'a', '%', 'b', ')']\n",
            "Tokenized C++ Code: ['return', '!', 'b', '?', 'a', ':', 'gcd', '(', 'b', ',', 'a', '%', 'b', ')', ';']\n",
            "--------------------------------------------------\n",
            "Index 2:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['}']\n",
            "--------------------------------------------------\n",
            "Index 3:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['int', 'main', '(', ')', '{']\n",
            "--------------------------------------------------\n",
            "Index 4:\n",
            "Tokenized Pseudocode: ['n', ',', 'nn', ',', 'ans', '=', 'integers', 'with', 'ans', '=0']\n",
            "Tokenized C++ Code: ['int', 'n', ',', 'nn', ',', 'ans', '=', '0', ';']\n",
            "--------------------------------------------------\n",
            "Index 5:\n",
            "Tokenized Pseudocode: ['Read', 'n']\n",
            "Tokenized C++ Code: ['cin', '>', '>', 'n', ';']\n",
            "--------------------------------------------------\n",
            "Index 6:\n",
            "Tokenized Pseudocode: ['for', 'i=2', 'to', 'n-1', 'execute']\n",
            "Tokenized C++ Code: ['for', '(', 'int', 'i', '=', '2', ';', 'i', '<', '=', 'n', '-', '1', ';', '++i', ')', '{']\n",
            "--------------------------------------------------\n",
            "Index 7:\n",
            "Tokenized Pseudocode: ['set', 'nn', 'to', 'n']\n",
            "Tokenized C++ Code: ['nn', '=', 'n', ';']\n",
            "--------------------------------------------------\n",
            "Index 8:\n",
            "Tokenized Pseudocode: ['while', 'nn', 'is', 'not', 'equal', 'to', '0', ',', 'set', 'ans', 'to', 'ans', '+', 'nn', '%', 'i', ',', 'and', 'also', 'set', 'nn=', 'nn/i']\n",
            "Tokenized C++ Code: ['while', '(', 'nn', ')', 'ans', '+=', 'nn', '%', 'i', ',', 'nn', '/=', 'i', ';']\n",
            "--------------------------------------------------\n",
            "Index 9:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['}']\n",
            "--------------------------------------------------\n",
            "Index 10:\n",
            "Tokenized Pseudocode: ['set', 'o', 'to', 'gcd', '(', 'ans', ',', 'n-2', ')']\n",
            "Tokenized C++ Code: ['int', 'o', '=', 'gcd', '(', 'ans', ',', 'n', '-', '2', ')', ';']\n",
            "--------------------------------------------------\n",
            "\n",
            "Samples from index 20-30:\n",
            "Index 20:\n",
            "Tokenized Pseudocode: ['for', 'i', '=', '2', 'to', 'n', '-', '1', 'inclusive']\n",
            "Tokenized C++ Code: ['for', '(', 'int', 'i', '=', '2', ';', 'i', '<', '=', 'n', '-', '1', ';', '++i', ')', '{']\n",
            "--------------------------------------------------\n",
            "Index 21:\n",
            "Tokenized Pseudocode: ['set', 'nn', 'to', 'n']\n",
            "Tokenized C++ Code: ['nn', '=', 'n', ';']\n",
            "--------------------------------------------------\n",
            "Index 22:\n",
            "Tokenized Pseudocode: ['while', 'nn', 'increment', 'ans', 'by', '(', 'nn', '%', 'i', ')', 'and', 'set', 'nn', 'to', 'nn', '/', 'i', ';']\n",
            "Tokenized C++ Code: ['while', '(', 'nn', ')', 'ans', '+=', 'nn', '%', 'i', ',', 'nn', '/=', 'i', ';']\n",
            "--------------------------------------------------\n",
            "Index 23:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['}']\n",
            "--------------------------------------------------\n",
            "Index 24:\n",
            "Tokenized Pseudocode: ['integer', 'o', '=', 'gcd', 'of', 'ans', 'and', 'n', '-', '2']\n",
            "Tokenized C++ Code: ['int', 'o', '=', 'gcd', '(', 'ans', ',', 'n', '-', '2', ')', ';']\n",
            "--------------------------------------------------\n",
            "Index 25:\n",
            "Tokenized Pseudocode: ['print', 'ans', '/', 'o', ',', '``', '/', \"''\", ',', '(', 'n', '-', '2', ')', '/', 'o', 'and', '``', '\\\\n', \"''\"]\n",
            "Tokenized C++ Code: ['cout', '<', '<', 'ans', '/', 'o', '<', '<', '``', '/', \"''\", '<', '<', '(', 'n', '-', '2', ')', '/', 'o', '<', '<', '``', '\\\\n', \"''\", ';']\n",
            "--------------------------------------------------\n",
            "Index 26:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['return', '0', ';']\n",
            "--------------------------------------------------\n",
            "Index 27:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['}']\n",
            "--------------------------------------------------\n",
            "Index 28:\n",
            "Tokenized Pseudocode: ['nan']\n",
            "Tokenized C++ Code: ['int', 'main', '(', ')', '{']\n",
            "--------------------------------------------------\n",
            "Index 29:\n",
            "Tokenized Pseudocode: ['let', 'a', 'and', 'b', 'be', 'strings']\n",
            "Tokenized C++ Code: ['string', 'a', ',', 'b', ';']\n",
            "--------------------------------------------------\n",
            "Index 30:\n",
            "Tokenized Pseudocode: ['n', '=', 'integer']\n",
            "Tokenized C++ Code: ['int', 'n', ';']\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Print samples from index 1-10\n",
        "print(\"Samples from index 1-10:\")\n",
        "for i in range(1, 11):\n",
        "    print(f\"Index {i}:\")\n",
        "    print(\"Tokenized Pseudocode:\", df[\"text_tokens\"].iloc[i])\n",
        "    print(\"Tokenized C++ Code:\", df[\"code_tokens\"].iloc[i])\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# Print samples from index 20-30\n",
        "print(\"\\nSamples from index 20-30:\")\n",
        "for i in range(20, 31):\n",
        "    print(f\"Index {i}:\")\n",
        "    print(\"Tokenized Pseudocode:\", df[\"text_tokens\"].iloc[i])\n",
        "    print(\"Tokenized C++ Code:\", df[\"code_tokens\"].iloc[i])\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenized pseudocode and C++ code to CSV\n",
        "output_file = \"tokenized_spoc.csv\"\n",
        "df[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Tokenized data saved to {output_file}\")\n"
      ],
      "metadata": {
        "id": "Sd8I0TttbCaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9961cfb3-8f87-4fb9-ba89-98bc6e942b33"
      },
      "id": "Sd8I0TttbCaZ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized data saved to tokenized_spoc.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add start and end tokens to tokenized C++ code\n",
        "df[\"code_tokens\"] = df[\"code_tokens\"].apply(lambda tokens: [\"<start>\"] + tokens + [\"<end>\"])\n",
        "\n",
        "# Save updated tokenized data to CSV\n",
        "output_file = \"tokenized_spoc_with_tokens.csv\"\n",
        "df[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Updated tokenized data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HAGLVzqXEQy",
        "outputId": "e58e32aa-af91-42c5-a3f3-9bdb897f3595"
      },
      "id": "-HAGLVzqXEQy",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated tokenized data saved to tokenized_spoc_with_tokens.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make \"text_tokens\" and \"code_tokens\" length same by padding with \"<pad>\"\n",
        "max_len = max(df[\"text_tokens\"].apply(len).max(), df[\"code_tokens\"].apply(len).max())\n",
        "\n",
        "df[\"text_tokens\"] = df[\"text_tokens\"].apply(lambda tokens: tokens + [\"<pad>\"] * (max_len - len(tokens)))\n",
        "df[\"code_tokens\"] = df[\"code_tokens\"].apply(lambda tokens: tokens + [\"<pad>\"] * (max_len - len(tokens)))\n",
        "\n",
        "# Save padded tokenized data to CSV\n",
        "output_file = \"tokenized_spoc_padded.csv\"\n",
        "df[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Padded tokenized data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tRCwKS0X25B",
        "outputId": "ede51d9e-6447-443b-f104-c9d626b6e21a"
      },
      "id": "6tRCwKS0X25B",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded tokenized data saved to tokenized_spoc_padded.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define special tokens with fixed indices\n",
        "vocab = {\n",
        "    \"<unk>\": 0,\n",
        "    \"<pad>\": 1,\n",
        "    \"<start>\": 2,\n",
        "    \"<end>\": 3\n",
        "}\n",
        "\n",
        "# Assign indices to other tokens\n",
        "for column in [\"text_tokens\", \"code_tokens\"]:\n",
        "    for tokens in df[column]:\n",
        "        for token in tokens:\n",
        "            if token not in vocab:\n",
        "                vocab[token] = len(vocab)\n",
        "\n",
        "# Save vocabulary to JSON\n",
        "vocab_file = \"vocabulary.json\"\n",
        "with open(vocab_file, \"w\") as f:\n",
        "    json.dump(vocab, f, indent=4)\n",
        "\n",
        "print(f\"Vocabulary saved to {vocab_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4hbVXb5YI4-",
        "outputId": "f01dbd92-2e80-4b46-f0e2-c473a4b9132c"
      },
      "id": "r4hbVXb5YI4-",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary saved to vocabulary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load vocabulary\n",
        "with open(\"vocabulary.json\", \"r\") as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "# Load tokenized data\n",
        "df = pd.read_csv(\"/content/tokenized_spoc_padded.csv\")\n",
        "\n",
        "# Convert string tokens to lists\n",
        "df[\"text_tokens\"] = df[\"text_tokens\"].apply(eval)\n",
        "df[\"code_tokens\"] = df[\"code_tokens\"].apply(eval)\n",
        "\n",
        "# Convert tokens to sequences using vocabulary\n",
        "df[\"text_sequences\"] = df[\"text_tokens\"].apply(lambda tokens: [vocab.get(token, vocab[\"<unk>\"]) for token in tokens])\n",
        "df[\"code_sequences\"] = df[\"code_tokens\"].apply(lambda tokens: [vocab.get(token, vocab[\"<unk>\"]) for token in tokens])\n",
        "\n",
        "# Save sequences to CSV\n",
        "output_file = \"tokenized_sequences.csv\"\n",
        "df[[\"text_sequences\", \"code_sequences\"]].to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Tokenized sequences saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TEFKw4KY6VO",
        "outputId": "4ae63ad3-d214-4917-d570-a8ab0ef4dc9e"
      },
      "id": "_TEFKw4KY6VO",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sequences saved to tokenized_sequences.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "import ast\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DataLoad(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "        self.inputs = [ast.literal_eval(x) for x in df['text_sequences']]\n",
        "        self.outputs = [ast.literal_eval(x) for x in df['code_sequences']]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_tensor = torch.tensor(self.inputs[idx], dtype=torch.int64)\n",
        "        output_tensor = torch.tensor(self.outputs[idx], dtype=torch.int64)\n",
        "        return input_tensor, output_tensor\n",
        "\n",
        "def Add_Pad(batch):\n",
        "    inputs, outputs = zip(*batch)\n",
        "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    outputs = pad_sequence(outputs, batch_first=True, padding_value=0)\n",
        "    return inputs, outputs\n",
        "\n",
        "# Load dataset and dataloader\n",
        "dataset = DataLoad('/content/tokenized_sequences.csv')\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=Add_Pad)\n",
        "\n",
        "# Iterate with progress bar\n",
        "data_iter = iter(dataloader)\n",
        "for batch in tqdm(dataloader, desc=\"Loading Batches\"):\n",
        "    features, labels = batch  # Get a batch of data\n",
        "    break  # Remove this if you want to iterate over all batches\n",
        "\n",
        "print(\"Batch loaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_ySZqiqaHUD",
        "outputId": "a1e13dd4-7a33-4ce1-9d00-19d4330b82ca"
      },
      "id": "8_ySZqiqaHUD",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading Batches:   0%|          | 0/1704 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(features)\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeXmffD0bl4E",
        "outputId": "8a548211-73ca-4305-a74e-611574d70658"
      },
      "id": "BeXmffD0bl4E",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 21,  24, 235,  ...,   1,   1,   1],\n",
            "        [ 34,  88,  68,  ...,   1,   1,   1],\n",
            "        [ 34,  23,  31,  ...,   1,   1,   1],\n",
            "        ...,\n",
            "        [102,  55,  41,  ...,   1,   1,   1],\n",
            "        [ 20,   1,   1,  ...,   1,   1,   1],\n",
            "        [ 16,  13, 159,  ...,   1,   1,   1]])\n",
            "tensor([[  2, 235,  21,  ...,   1,   1,   1],\n",
            "        [  2, 266,  24,  ...,   1,   1,   1],\n",
            "        [  2,  23,  24,  ...,   1,   1,   1],\n",
            "        ...,\n",
            "        [  2, 235,  41,  ...,   1,   1,   1],\n",
            "        [  2, 498,   3,  ...,   1,   1,   1],\n",
            "        [  2,  16,  13,  ...,   1,   1,   1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "# Transformer Hyperparameters\n",
        "class Config:\n",
        "    vocab_size = 12006  # Adjust based on vocabulary.json\n",
        "    max_length = 100  # Adjust based on sequence length\n",
        "    embed_dim = 256\n",
        "    num_heads = 8\n",
        "    num_layers =2\n",
        "    feedforward_dim = 512\n",
        "    dropout = 0.1\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=100):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)  # Shape: (1, max_len, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "# Transformer Model\n",
        "class PseudoCodeTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(PseudoCodeTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(config.embed_dim, config.max_length)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=config.embed_dim,\n",
        "            nhead=config.num_heads,\n",
        "            num_encoder_layers=config.num_layers,\n",
        "            num_decoder_layers=config.num_layers,\n",
        "            dim_feedforward=config.feedforward_dim,\n",
        "            dropout=config.dropout\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(config.embed_dim, config.vocab_size)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(config.device)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.embedding(src) * math.sqrt(config.embed_dim)\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(config.embed_dim)\n",
        "\n",
        "        src_emb = self.positional_encoding(src_emb)\n",
        "        tgt_emb = self.positional_encoding(tgt_emb)\n",
        "\n",
        "        src_mask = self.generate_square_subsequent_mask(src.size(1))\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
        "\n",
        "        out = self.transformer(src_emb.permute(1, 0, 2), tgt_emb.permute(1, 0, 2),\n",
        "                               src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "        out = self.fc_out(out.permute(1, 0, 2))  # Convert back to batch-first\n",
        "        return out\n",
        "\n",
        "# Initialize Model\n",
        "model = PseudoCodeTransformer(config).to(config.device)\n",
        "print(\"Model initialized successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azPgilarcWXf",
        "outputId": "85037ea4-4eea-4c07-f178-2e06a8db920d"
      },
      "id": "azPgilarcWXf",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, pseudocode_tokens, vocab, device, max_length=50):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert pseudocode tokens to numerical indices\n",
        "    input_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in pseudocode_tokens]\n",
        "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    # Start token for generation\n",
        "    output_ids = [vocab[\"<start>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        output_tensor = torch.tensor(output_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(input_tensor, output_tensor)\n",
        "\n",
        "        # Select the most probable token\n",
        "        next_token_id = predictions.argmax(dim=-1)[:, -1].item()\n",
        "        output_ids.append(next_token_id)\n",
        "\n",
        "        # Stop if end token is generated\n",
        "        if next_token_id == vocab[\"<end>\"]:\n",
        "            break\n",
        "\n",
        "    # Convert token indices back to words\n",
        "    id_to_token = {idx: token for token, idx in vocab.items()}\n",
        "    translated_code = [id_to_token.get(idx, \"<unk>\") for idx in output_ids[1:]]  # Exclude <start> token\n",
        "\n",
        "    return \" \".join(translated_code)\n"
      ],
      "metadata": {
        "id": "2XsYwb5jLxAT"
      },
      "id": "2XsYwb5jLxAT",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load vocabulary\n",
        "with open(\"vocabulary.json\", \"r\") as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "# Ensure vocab is a dictionary\n",
        "print(f\"âœ… Vocabulary loaded with {len(vocab)} tokens\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJBeh_zNL6ZM",
        "outputId": "ea8069da-fe38-475a-d627-7b136f6651bf"
      },
      "id": "OJBeh_zNL6ZM",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Vocabulary loaded with 6656 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ”¹ Using device: {device}\")\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "# Loss Function & Optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=1)  # Ignore padding token\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
        "\n",
        "# Create directory to save models\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch in progress_bar:\n",
        "        src, tgt = batch\n",
        "        src, tgt = src.to(device), tgt.to(device)  # Move batch to GPU\n",
        "\n",
        "        tgt_input = tgt[:, :-1]  # Remove <end> token\n",
        "        tgt_output = tgt[:, 1:]  # Shifted version\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "\n",
        "        loss = criterion(output.view(-1, config.vocab_size), tgt_output.contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save Model Checkpoint\n",
        "    torch.save(model.state_dict(), f\"checkpoints/transformer_epoch_{epoch+1}.pth\")\n",
        "    print(f\"âœ… Model saved: checkpoints/transformer_epoch_{epoch+1}.pth\")\n",
        "\n",
        "    # Print Example Prediction\n",
        "    model.eval()\n",
        "    example_pseudocode = [\"create\", \"integer\", \"x\"]\n",
        "    translated_code = translate(model, example_pseudocode, vocab, device)\n",
        "    print(f\"ðŸ”¹ Example Prediction (Pseudocode â†’ C++): {translated_code}\\n\")\n"
      ],
      "metadata": {
        "id": "SUOIS04idMXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47418b39-61ca-42e3-e9a1-1c412c5cd627"
      },
      "id": "SUOIS04idMXB",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1704/1704 [02:42<00:00, 10.49it/s, loss=0.361]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 1.2264\n",
            "âœ… Model saved: checkpoints/transformer_epoch_1.pth\n",
            "ðŸ”¹ Example Prediction (Pseudocode â†’ C++): int x x x x x = int x x x x x , int x = int x x x ; <end>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_pseudocode = [\"for\", \"i\", \"=\", \"0\", \"to\", \"size\", \"of\", \"ans\", \"exclusive\", \",\", \"print\", \"ans\", \"[\", \"i\", \"]\", \"print\", \"newline\"]\n",
        "translated_code = translate(model, example_pseudocode, vocab, device)\n",
        "print(f\"ðŸ”¹ Example Prediction (Pseudocode â†’ C++): {translated_code}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5pxkdcUL5Gd",
        "outputId": "1a37ec56-a808-4c97-b630-7440690c418b"
      },
      "id": "D5pxkdcUL5Gd",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Example Prediction (Pseudocode â†’ C++): for ( int i = 0 ; i < ans = 0 ; i++ ) { ans for ( ans = abs ( ans ( ans ( ans , ans ( ans ) ; } <end>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for (int i = 0; i < ans.size(); i++) { cout << ans[i] << endl; }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "hh7c0AziPqG5",
        "outputId": "aef6a90c-c902-4351-f583-52441ae26884"
      },
      "id": "hh7c0AziPqG5",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-26-ee7d37ebdb44>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-ee7d37ebdb44>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for (int i = 0; i < ans.size(); i++) { cout << ans[i] << endl; }\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ”¹ Using device: {device}\")\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "model = PseudoCodeTransformer(config).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/checkpoints/transformer_epoch_1.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Run translation on example pseudocode\n",
        "example_pseudocode = [\"for\", \"i\", \"=\", \"0\", \"to\", \"size\", \"of\", \"ans\", \"exclusive\", \",\", \"print\", \"ans\", \"[\", \"i\", \"]\", \"print\", \"newline\"]\n",
        "translated_code = translate(model, example_pseudocode, vocab, device)\n",
        "\n",
        "print(f\"ðŸ”¹ Example Prediction (Pseudocode â†’ C++): {translated_code}\\n\")\n"
      ],
      "metadata": {
        "id": "t-xzQokaPy_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "706ba0a4-5c2b-4bf8-cbe6-0d4c09a8e172"
      },
      "id": "t-xzQokaPy_E",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Using device: cuda\n",
            "ðŸ”¹ Example Prediction (Pseudocode â†’ C++): for ( int i = 0 ; i < ans = 0 ; i++ ) { ans for ( ans = abs ( ans ( ans ( ans , ans ( ans ) ; } <end>\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-6b8e2ac4d067>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/checkpoints/transformer_epoch_1.pth\", map_location=device))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C++ CODE TO PSEUDOCODE**"
      ],
      "metadata": {
        "id": "CrE_8fkdRdfQ"
      },
      "id": "CrE_8fkdRdfQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pandas as pd\n",
        "import ast\n",
        "import json\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Transformer Hyperparameters\n",
        "class Config:\n",
        "    vocab_size = 12006  # Adjust based on vocabulary.json\n",
        "    max_length = 100\n",
        "    embed_dim = 256\n",
        "    num_heads = 8\n",
        "    num_layers = 2\n",
        "    feedforward_dim = 512\n",
        "    dropout = 0.1\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=100):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "# Transformer Model\n",
        "class CPPtoPseudoTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CPPtoPseudoTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
        "        self.positional_encoding = PositionalEncoding(config.embed_dim, config.max_length)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=config.embed_dim,\n",
        "            nhead=config.num_heads,\n",
        "            num_encoder_layers=config.num_layers,\n",
        "            num_decoder_layers=config.num_layers,\n",
        "            dim_feedforward=config.feedforward_dim,\n",
        "            dropout=config.dropout\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(config.embed_dim, config.vocab_size)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(config.device)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.embedding(src) * math.sqrt(config.embed_dim)\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(config.embed_dim)\n",
        "\n",
        "        src_emb = self.positional_encoding(src_emb)\n",
        "        tgt_emb = self.positional_encoding(tgt_emb)\n",
        "\n",
        "        src_mask = self.generate_square_subsequent_mask(src.size(1))\n",
        "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
        "\n",
        "        out = self.transformer(src_emb.permute(1, 0, 2), tgt_emb.permute(1, 0, 2),\n",
        "                               src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "        out = self.fc_out(out.permute(1, 0, 2))\n",
        "        return out\n",
        "\n",
        "# Initialize Model\n",
        "model = CPPtoPseudoTransformer(config).to(config.device)\n",
        "print(\"ðŸš€ C++ â†’ Pseudocode Model initialized!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE-UUuDGI5Az",
        "outputId": "e8f388f2-cd12-44ad-c33e-33a1eb30c4f5"
      },
      "id": "yE-UUuDGI5Az",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ C++ â†’ Pseudocode Model initialized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load Vocabulary\n",
        "with open(\"vocabulary.json\", \"r\") as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "print(f\"âœ… Vocabulary loaded with {len(vocab)} tokens\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdWsUGr4KHM1",
        "outputId": "b230e17a-1e2d-4b4e-c97a-80c2f36496f1"
      },
      "id": "CdWsUGr4KHM1",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Vocabulary loaded with 6656 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate2(model, cpp_tokens, vocab, device, max_length=50):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert C++ tokens to numerical indices\n",
        "    input_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in cpp_tokens]\n",
        "    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "    output_ids = [vocab[\"<start>\"]]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        output_tensor = torch.tensor(output_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(input_tensor, output_tensor)\n",
        "\n",
        "        # Select the most probable token\n",
        "        next_token_id = predictions.argmax(dim=-1)[:, -1].item()\n",
        "\n",
        "        if next_token_id == vocab[\"<pad>\"]:  # Ignore <pad> tokens\n",
        "            continue\n",
        "\n",
        "        output_ids.append(next_token_id)\n",
        "\n",
        "        if next_token_id == vocab[\"<end>\"]:  # Stop if <end> is generated\n",
        "            break\n",
        "\n",
        "    # Convert token indices back to words\n",
        "    id_to_token = {idx: token for token, idx in vocab.items()}\n",
        "    translated_pseudocode = [id_to_token.get(idx, \"<unk>\") for idx in output_ids[1:]]  # Exclude <start>\n",
        "\n",
        "    return \" \".join(translated_pseudocode)\n"
      ],
      "metadata": {
        "id": "BEEQ_zNHKQRY"
      },
      "id": "BEEQ_zNHKQRY",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Class\n",
        "class CPPToPseudoDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        df = pd.read_csv(file_path)\n",
        "        self.inputs = [ast.literal_eval(x) for x in df['code_sequences']]\n",
        "        self.outputs = [ast.literal_eval(x) for x in df['text_sequences']]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_tensor = torch.tensor(self.inputs[idx], dtype=torch.int64)\n",
        "        output_tensor = torch.tensor([vocab[\"<start>\"]] + self.outputs[idx] + [vocab[\"<end>\"]], dtype=torch.int64)\n",
        "        return input_tensor, output_tensor\n",
        "\n",
        "# Padding Function\n",
        "def Add_Pad(batch):\n",
        "    inputs, outputs = zip(*batch)\n",
        "    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    outputs = pad_sequence(outputs, batch_first=True, padding_value=0)\n",
        "    return inputs, outputs\n",
        "\n",
        "# Load Dataset\n",
        "dataset = CPPToPseudoDataset(\"tokenized_sequences.csv\")\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=Add_Pad)\n",
        "\n",
        "print(f\"âœ… Loaded {len(dataset)} examples for training\")\n",
        "\n",
        "# Training Configuration\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n",
        "\n",
        "# Create directory to save models\n",
        "os.makedirs(\"checkpoints\", exist_ok=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3HIGFZCKNaB",
        "outputId": "b8b2f62b-9cd9-4104-909f-749f31e3596b"
      },
      "id": "t3HIGFZCKNaB",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded 109002 examples for training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch in progress_bar:\n",
        "        src, tgt = batch\n",
        "        src, tgt = src.to(config.device), tgt.to(config.device)\n",
        "\n",
        "        tgt_input = tgt[:, :-1]\n",
        "        tgt_output = tgt[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_input)\n",
        "\n",
        "        loss = criterion(output.view(-1, config.vocab_size), tgt_output.contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save Model Checkpoint\n",
        "    torch.save(model.state_dict(), f\"checkpoints/cpp_to_pseudo_epoch_{epoch+1}.pth\")\n",
        "    print(f\"âœ… Model saved: checkpoints/cpp_to_pseudo_epoch_{epoch+1}.pth\")\n",
        "\n",
        "    # Print Example Prediction\n",
        "    model.eval()\n",
        "    example_cpp = [\"int\", \"main\", \"(\", \")\", \"{\", \"return\", \"0\", \";\", \"}\"]\n",
        "    translated_pseudocode = translate2(model, example_cpp, vocab, config.device)\n",
        "    print(f\"ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): {translated_pseudocode}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXd9vz5EKT8u",
        "outputId": "10d44bed-7ddf-470e-910b-1b9503f2cab0"
      },
      "id": "QXd9vz5EKT8u",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1704/1704 [02:41<00:00, 10.57it/s, loss=1.33]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 1.0850\n",
            "âœ… Model saved: checkpoints/cpp_to_pseudo_epoch_1.pth\n",
            "ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): declare integer called with integer arguments , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1704/1704 [02:43<00:00, 10.43it/s, loss=1.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/5], Loss: 1.0870\n",
            "âœ… Model saved: checkpoints/cpp_to_pseudo_epoch_2.pth\n",
            "ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): declare integer called with integer arguments , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1704/1704 [02:43<00:00, 10.43it/s, loss=1.88]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/5], Loss: 1.0857\n",
            "âœ… Model saved: checkpoints/cpp_to_pseudo_epoch_3.pth\n",
            "ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): declare integer called with integer arguments , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1704/1704 [02:43<00:00, 10.42it/s, loss=0.686]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/5], Loss: 1.0851\n",
            "âœ… Model saved: checkpoints/cpp_to_pseudo_epoch_4.pth\n",
            "ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): declare integer called with integer arguments , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1704/1704 [02:43<00:00, 10.43it/s, loss=1.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/5], Loss: 1.0853\n",
            "âœ… Model saved: checkpoints/cpp_to_pseudo_epoch_5.pth\n",
            "ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): declare integer called with integer arguments , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return 0 , return\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"ðŸ”¹ Using device: {device}\")\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "model = PseudoCodeTransformer(config).to(device)\n",
        "model.load_state_dict(torch.load(\"/content/checkpoints/cpp_to_pseudo_epoch_4.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "example_cpp = [\"int\", \"a\",\"=\", \"10\",\";\"]\n",
        "translated_pseudocode = translate2(model, example_cpp, vocab, config.device)\n",
        "print(f\"ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): {translated_pseudocode}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfjIcOTkK33z",
        "outputId": "0606dde6-28ef-40ea-b2d6-b03ab543f36c"
      },
      "id": "tfjIcOTkK33z",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-62b96cae03d2>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/checkpoints/cpp_to_pseudo_epoch_4.pth\", map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”¹ Example Prediction (C++ â†’ Pseudocode): create integer a integer a with a = 10 = 10 = 10 = 10 = 10 = 10 10 = 10 = 10 10 = 10 10 = 10 = 10 = 10 = 10 = 10 10 = 10 = 10 10 = 10 10 = 10 =\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBmHMGCJM3Cw"
      },
      "id": "vBmHMGCJM3Cw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}